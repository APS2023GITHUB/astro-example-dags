"""
CloudIDE Quickstart
DAG auto-generated by Astro Cloud IDE. 
This DAG collects data on every endangered species by country from a public API, then transforms the dataset to create a cleaned and aggregate dataset of the total number of endangered species in each country.
This DAG was created using the Astro SDK, which both eliminates boilerplate code and allows for easier passing of data between python and SQL tasks. 
"""

from airflow.decorators import dag
from astro import sql as aql
from astro.table import Table, Metadata
import pandas as pd
import pendulum

import pandas as pd

"""
Part 1. Task Definition
Before we set the order of our dag, we'll need to first create our tasks. The Astro SDK makes this easy by allowing us to define our tasks just as we would a python function or a sql script. 
Then, all we need to do is add the corresponding AQL decorater, with @aql.dataframe being used for python tasks that return a dataframe, and @aql.run_raw_sql for tasks where we'll be executing sql commands in a database.
Here, we'll create a python task called load_species_status_data_func to call an API and collect a dataframe of endangered species. 
Then, we'll create a sql task calle species_by_country_func to extract only the rows we need from that dataframe, and return them as a dataset to be used in the next task. 
Finally, we'll create a python task transform_endangered_species_func that takes the cleaned dataset and runs some transformations on it to aggregate the endangered species by country
"""

@aql.dataframe(task_id="load_species_status_data")
def load_species_status_data_func():
    return pd.read_csv("https://raw.githubusercontent.com/astronomer/cli-cloud-ide-workshop/main/include/data/country_species_status_cleaned.csv", on_bad_lines='skip', nrows=20)

"""
For this task to work, you'll need to create a connection in the Airflow UI called "postgres" with the following fields
Host: postgres
Login: postgres
Password: postgres
Port: 5432
This will all you to use the default Airflow backend database to store your values. This database has a very limited capacity, so for production data you'll want to use an external database. 
"""

@aql.run_raw_sql(conn_id="postgres", task_id="species_by_country", results_format="pandas_dataframe")
def species_by_country_func(load_species_status_data: Table):
    return """
    SELECT 
        "iucn",
        "IUCN Category",
        "spec",
        "Species",
        "cou",
        "Country",
        "Value"
    FROM {{ load_species_status_data }};
    """

@aql.dataframe(task_id="transform_endangered_species")
def transform_endangered_species_func(species_by_country: pd.DataFrame):
    all_species = species_by_country
    
    endangered_species = all_species[(all_species['Species']!='Fish') & (all_species['IUCN Category'] == 'Number of critically endangered species')]
    endangered_species_agg = endangered_species[['cou', 'Country', 'Value']].groupby(['cou', 'Country']).sum().reset_index()
    
    endangered_species_agg.rename(
       columns = { 'Value': 'endangered_species' },
       inplace = True  
    )
    
    return endangered_species_agg


@dag(
    schedule="0 0 * * *",
    start_date=pendulum.from_format("2023-06-12", "YYYY-MM-DD").in_tz("UTC"),
    catchup=False,
)
def CloudIDEQuickstart():
    '''
    DAG Definition: 
    Here in the DAG definition section, we want to define the relationships between the tasks so that they can pass data between one another. 
    Data is passed between the tasks by using each upstream task as an argument to the next downstream task. With the Taskflow API, this has the result of passing whatever data is returned from the upstream task, into the downstream task.
    The Astro SDK makes this even easier by allowing us to pass data between python and SQL tasks with no intermediary task needed! 
    So in species_by_country_func(load_species_status_data,) the loaded species status data is passed to the species_by_country function as a pandas dataframe. 
    Then, the resulting dataset from that transformation is passed to the transform_endangered_species_func task like so: transform_endangered_species_func(species_by_country,)
    '''
    load_species_status_data = load_species_status_data_func()

    species_by_country = species_by_country_func(load_species_status_data,)

    transform_endangered_species = transform_endangered_species_func(species_by_country,)

#Finally, we'll define the dag object here by calling the DAG function CloudIDEQuickstart()
dag_obj = CloudIDEQuickstart()
